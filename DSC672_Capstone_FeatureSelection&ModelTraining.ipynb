{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\guy.dor\\\\Documents\\\\672train'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Guy Dor\n",
    "###DSC 672\n",
    "###Final Project Code\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inf_adj_salary</th>\n",
       "      <th>Part_Time</th>\n",
       "      <th>Full_Time</th>\n",
       "      <th>White</th>\n",
       "      <th>Hispanic_Latino</th>\n",
       "      <th>NativeAmerican</th>\n",
       "      <th>AfricanAmerican</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>...</th>\n",
       "      <th>low_grade</th>\n",
       "      <th>high_grade</th>\n",
       "      <th>pct_admin</th>\n",
       "      <th>Year</th>\n",
       "      <th>mean inc</th>\n",
       "      <th>EducationECI</th>\n",
       "      <th>Avg Reading</th>\n",
       "      <th>Avg Math</th>\n",
       "      <th>Enrollment</th>\n",
       "      <th>ln_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4095.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>73051.933702</td>\n",
       "      <td>118.8</td>\n",
       "      <td>82.495238</td>\n",
       "      <td>87.366667</td>\n",
       "      <td>19711</td>\n",
       "      <td>8.317522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14493.885389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>40437.266151</td>\n",
       "      <td>116.7</td>\n",
       "      <td>69.330174</td>\n",
       "      <td>79.221914</td>\n",
       "      <td>24224</td>\n",
       "      <td>9.581482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17687.340666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>35437.889054</td>\n",
       "      <td>107.9</td>\n",
       "      <td>66.520105</td>\n",
       "      <td>77.523133</td>\n",
       "      <td>23424</td>\n",
       "      <td>9.780604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77116.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>22113.411765</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.764943</td>\n",
       "      <td>53.427586</td>\n",
       "      <td>20630</td>\n",
       "      <td>11.253072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28319.438790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>50011.637573</td>\n",
       "      <td>115.3</td>\n",
       "      <td>76.994350</td>\n",
       "      <td>80.787271</td>\n",
       "      <td>72770</td>\n",
       "      <td>10.251304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   inf_adj_salary  Part_Time  Full_Time  White  Hispanic_Latino  \\\n",
       "0     4095.000000          0          0      0                0   \n",
       "1    14493.885389          0          0      0                0   \n",
       "2    17687.340666          0          0      0                0   \n",
       "3    77116.444444          0          0      0                0   \n",
       "4    28319.438790          0          0      1                0   \n",
       "\n",
       "   NativeAmerican  AfricanAmerican  Asian  Female  Male    ...      low_grade  \\\n",
       "0               0                0      0       0     1    ...             -3   \n",
       "1               0                1      0       1     0    ...             -1   \n",
       "2               0                1      0       1     0    ...             -1   \n",
       "3               0                1      0       0     1    ...              6   \n",
       "4               0                0      0       0     1    ...              9   \n",
       "\n",
       "   high_grade  pct_admin  Year      mean inc  EducationECI  Avg Reading  \\\n",
       "0           5        0.0  2012  73051.933702         118.8    82.495238   \n",
       "1           3        0.0  2011  40437.266151         116.7    69.330174   \n",
       "2           3        0.0  2007  35437.889054         107.9    66.520105   \n",
       "3           8        0.0  2005  22113.411765         100.0    46.764943   \n",
       "4          12        0.0  2010  50011.637573         115.3    76.994350   \n",
       "\n",
       "    Avg Math  Enrollment   ln_salary  \n",
       "0  87.366667        19711   8.317522  \n",
       "1  79.221914        24224   9.581482  \n",
       "2  77.523133        23424   9.780604  \n",
       "3  53.427586        20630  11.253072  \n",
       "4  80.787271        72770  10.251304  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "file1= pd.read_csv('3train.csv', delimiter=',')\n",
    "file1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inf_adj_salary</th>\n",
       "      <th>Part_Time</th>\n",
       "      <th>Full_Time</th>\n",
       "      <th>White</th>\n",
       "      <th>Hispanic_Latino</th>\n",
       "      <th>NativeAmerican</th>\n",
       "      <th>AfricanAmerican</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>...</th>\n",
       "      <th>low_grade</th>\n",
       "      <th>high_grade</th>\n",
       "      <th>pct_admin</th>\n",
       "      <th>Year</th>\n",
       "      <th>mean inc</th>\n",
       "      <th>EducationECI</th>\n",
       "      <th>Avg Reading</th>\n",
       "      <th>Avg Math</th>\n",
       "      <th>Enrollment</th>\n",
       "      <th>ln_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "      <td>913489.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>64286.397705</td>\n",
       "      <td>0.022713</td>\n",
       "      <td>0.974782</td>\n",
       "      <td>0.849240</td>\n",
       "      <td>0.046541</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.084694</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.769995</td>\n",
       "      <td>0.230004</td>\n",
       "      <td>...</td>\n",
       "      <td>2.095008</td>\n",
       "      <td>7.041141</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>2007.485259</td>\n",
       "      <td>107362.546064</td>\n",
       "      <td>107.685859</td>\n",
       "      <td>68.781378</td>\n",
       "      <td>73.786965</td>\n",
       "      <td>169298.968787</td>\n",
       "      <td>11.012365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21720.087044</td>\n",
       "      <td>0.148987</td>\n",
       "      <td>0.156786</td>\n",
       "      <td>0.357815</td>\n",
       "      <td>0.210654</td>\n",
       "      <td>0.041104</td>\n",
       "      <td>0.278426</td>\n",
       "      <td>0.109984</td>\n",
       "      <td>0.420836</td>\n",
       "      <td>0.420835</td>\n",
       "      <td>...</td>\n",
       "      <td>5.173979</td>\n",
       "      <td>4.054916</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>2.858936</td>\n",
       "      <td>110100.897670</td>\n",
       "      <td>8.496458</td>\n",
       "      <td>11.251111</td>\n",
       "      <td>12.606226</td>\n",
       "      <td>300849.814998</td>\n",
       "      <td>0.356279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.042109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>16310.344830</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>14.300000</td>\n",
       "      <td>14.300000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>-3.167491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48424.440165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>42248.006380</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>61.894629</td>\n",
       "      <td>67.011224</td>\n",
       "      <td>11284.000000</td>\n",
       "      <td>10.787760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60552.697000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>56514.405360</td>\n",
       "      <td>111.300000</td>\n",
       "      <td>70.433333</td>\n",
       "      <td>76.397059</td>\n",
       "      <td>31752.000000</td>\n",
       "      <td>11.011269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>77773.815726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>104544.874615</td>\n",
       "      <td>115.300000</td>\n",
       "      <td>76.807071</td>\n",
       "      <td>82.856105</td>\n",
       "      <td>72211.000000</td>\n",
       "      <td>11.261560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>158514.909638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>790705.287714</td>\n",
       "      <td>118.800000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>855562.000000</td>\n",
       "      <td>11.973604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       inf_adj_salary      Part_Time      Full_Time          White  \\\n",
       "count   913489.000000  913489.000000  913489.000000  913489.000000   \n",
       "mean     64286.397705       0.022713       0.974782       0.849240   \n",
       "std      21720.087044       0.148987       0.156786       0.357815   \n",
       "min          0.042109       0.000000       0.000000       0.000000   \n",
       "25%      48424.440165       0.000000       1.000000       1.000000   \n",
       "50%      60552.697000       0.000000       1.000000       1.000000   \n",
       "75%      77773.815726       0.000000       1.000000       1.000000   \n",
       "max     158514.909638       1.000000       1.000000       1.000000   \n",
       "\n",
       "       Hispanic_Latino  NativeAmerican  AfricanAmerican          Asian  \\\n",
       "count    913489.000000   913489.000000    913489.000000  913489.000000   \n",
       "mean          0.046541        0.001692         0.084694       0.012246   \n",
       "std           0.210654        0.041104         0.278426       0.109984   \n",
       "min           0.000000        0.000000         0.000000       0.000000   \n",
       "25%           0.000000        0.000000         0.000000       0.000000   \n",
       "50%           0.000000        0.000000         0.000000       0.000000   \n",
       "75%           0.000000        0.000000         0.000000       0.000000   \n",
       "max           1.000000        1.000000         1.000000       1.000000   \n",
       "\n",
       "              Female           Male      ...            low_grade  \\\n",
       "count  913489.000000  913489.000000      ...        913489.000000   \n",
       "mean        0.769995       0.230004      ...             2.095008   \n",
       "std         0.420836       0.420835      ...             5.173979   \n",
       "min         0.000000       0.000000      ...            -3.000000   \n",
       "25%         1.000000       0.000000      ...            -3.000000   \n",
       "50%         1.000000       0.000000      ...            -1.000000   \n",
       "75%         1.000000       0.000000      ...             9.000000   \n",
       "max         1.000000       1.000000      ...            11.000000   \n",
       "\n",
       "          high_grade      pct_admin           Year       mean inc  \\\n",
       "count  913489.000000  913489.000000  913489.000000  913489.000000   \n",
       "mean        7.041141       0.000541    2007.485259  107362.546064   \n",
       "std         4.054916       0.015465       2.858936  110100.897670   \n",
       "min        -3.000000       0.000000    2003.000000   16310.344830   \n",
       "25%         4.000000       0.000000    2005.000000   42248.006380   \n",
       "50%         8.000000       0.000000    2008.000000   56514.405360   \n",
       "75%        12.000000       0.000000    2010.000000  104544.874615   \n",
       "max        12.000000       1.000000    2012.000000  790705.287714   \n",
       "\n",
       "        EducationECI    Avg Reading       Avg Math    Enrollment   \\\n",
       "count  913489.000000  913489.000000  913489.000000  913489.000000   \n",
       "mean      107.685859      68.781378      73.786965  169298.968787   \n",
       "std         8.496458      11.251111      12.606226  300849.814998   \n",
       "min        93.200000      14.300000      14.300000      92.000000   \n",
       "25%       100.000000      61.894629      67.011224   11284.000000   \n",
       "50%       111.300000      70.433333      76.397059   31752.000000   \n",
       "75%       115.300000      76.807071      82.856105   72211.000000   \n",
       "max       118.800000     100.000000     100.000000  855562.000000   \n",
       "\n",
       "           ln_salary  \n",
       "count  913489.000000  \n",
       "mean       11.012365  \n",
       "std         0.356279  \n",
       "min        -3.167491  \n",
       "25%        10.787760  \n",
       "50%        11.011269  \n",
       "75%        11.261560  \n",
       "max        11.973604  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Import Packages\n",
    "#\n",
    "#####################\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold, SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, mutual_info_classif, chi2\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import KBinsDiscretizer, scale\n",
    "\n",
    "#Handle annoying warnings\n",
    "import warnings, sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Global parameters\n",
    "#\n",
    "#####################\n",
    "\n",
    "target_idx=0                                        #Index of Target variable\n",
    "cross_val=1                                         #Control Switch for CV\n",
    "norm_target=0                                       #Normalize target switch\n",
    "norm_features=0                                     #Normalize features switch\n",
    "binning=0                                           #Control Switch for Bin Target\n",
    "bin_cnt=2                                           #If bin target, this sets number of classes\n",
    "feat_select=1                                       #Control Switch for Feature Selection\n",
    "fs_type=2                                           #Feature Selection type (1=Stepwise Backwards Removal, 2=Wrapper Select, 3=Univariate Selection, 4=Full-Blown Wrapper)\n",
    "lv_filter=0                                         #Control switch for low variance filter on features\n",
    "feat_start=1                                        #Start column of features\n",
    "k_cnt=10                                            #Number of 'Top k' best ranked features to select, only applies for fs_types 1 and 3\n",
    "gridsearch=0                                        #Enabling of Cross Validated GridSearch\n",
    "resampling=0                                        #Resampling Switch\n",
    "resampling_type=2                                   #Resampling using (1=SMOTE, 2=ADASYN, 3=RandomUnderSampler, 4=NearMiss, 5=ClusterCentroids, 51=ClusterCentroids+SMOTE)\n",
    "\n",
    "#Set global model parameters\n",
    "rand_st=1                                           #Set Random State variable for randomizing splits on runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Wrapper Feat Select Helper\n",
    "#\n",
    "##########################################\n",
    "\n",
    "#Recursive Function for searching thru feature space\n",
    "'''\n",
    "def feat_space_search(arr, curr_idx):\n",
    "    '''Setup currently as exhuastive search, but could be changed to use\n",
    "       greedy search, random search, genetic algorithms, etc. ... also\n",
    "       no regularization, so probably selects more features than necessary'''\n",
    "    global roll_idx, combo_ctr, best_score, sel_idx\n",
    "    \n",
    "    if curr_idx==feat_cnt:\n",
    "        #If end of feature array, roll thru combinations\n",
    "        roll_idx=roll_idx+1\n",
    "        print (\"Combos Searched so far:\", combo_ctr, \"Current Best Score:\", best_score)\n",
    "        for i in range(roll_idx, len(arr)):\n",
    "            arr[i]=0\n",
    "        if roll_idx<feat_cnt-1:\n",
    "            feat_space_search(arr, roll_idx+1)                                                                      #Recurse till end of rolls\n",
    "        \n",
    "    else:\n",
    "        #Else setup next feature combination and calc performance\n",
    "        arr[curr_idx]=1\n",
    "        data=data_np#_wrap                                                                                          #Temp array to hold data\n",
    "        temp_del=[i for i in range(len(arr)) if arr[i]==0]                                                          #Pick out features not in this combo, and remove\n",
    "        data = np.delete(data, temp_del, axis=1)\n",
    "        data_train, data_test, target_train, target_test = train_test_split(data, target_np, test_size=0.35)                \n",
    "\n",
    "        if binning==1:\n",
    "            if bin_cnt<=2:\n",
    "                scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "                scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5) \n",
    "                score = scores['test_roc_auc'].mean()                                                               #AUC\n",
    "            else:\n",
    "                sscorers = {'Accuracy': 'accuracy'}\n",
    "                scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5) \n",
    "                score = scores['test_Accuracy'].mean()                                                              #Accuracy\n",
    "            #print('Random Forest Acc/AUC:', curr_idx, feat_arr, len(data[0]), score)\n",
    "            if score>best_score:                                                                                    #Compare performance and update sel_idx and best_score, if needed\n",
    "                best_score=score\n",
    "                sel_idx=copy.deepcopy(arr) \n",
    "                \n",
    "        if binning==0:\n",
    "            scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'}\n",
    "            scores = cross_validate(rgr, data, target_np, scoring=scorers, cv=5)    \n",
    "            score = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']]).mean()                              #RMSE\n",
    "            #print('Random Forest RMSE:', curr_idx, feat_arr, len(data[0]), score)\n",
    "            if score<best_score:                                                                                    #Compare performance and update sel_idx and best_score, if needed\n",
    "                best_score=score\n",
    "                sel_idx=copy.deepcopy(arr) \n",
    "\n",
    "        #move to next feature index and recurse\n",
    "        combo_ctr+=1  \n",
    "        curr_idx+=1\n",
    "        feat_space_search(arr, curr_idx)                                                                            #Recurse till end of iteration for roll\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿inf_adj_salary', 'Part_Time', 'Full_Time', 'White', 'Hispanic_Latino', 'NativeAmerican', 'AfricanAmerican', 'Asian', 'Female', 'Male', 'Sp_Ed_Teacher', 'Junior_High_Middle_School', 'HighSchool', 'Elementary', 'Technical', 'Special_Education', 'Science', 'Math', 'Language', 'Humanities', 'Art', 'dst_3_zip', 'dst_zip', 'dst_zip_plus4', 'Num_Schools', 'months_employed', 'pct_emp', 'fte', 'dist_exp', 'state_exp', 'out_of_state_exp', 'IL_bac_coll', 'high_degre_cd', 'IL_adv_col', 'low_grade', 'high_grade', 'pct_admin', 'Year', 'mean inc', 'EducationECI', 'Avg Reading', 'Avg Math', 'Enrollment ', 'ln_salary']\n",
      "913489 913489\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Load Data\n",
    "#\n",
    "#####################\n",
    "\n",
    "file1= csv.reader(open('3train.csv'), delimiter=',')\n",
    "\n",
    "#Read Header Line\n",
    "header=next(file1)            \n",
    "\n",
    "#Read data\n",
    "data=[]\n",
    "target=[]\n",
    "for row in file1:\n",
    "    #Load Target\n",
    "    if row[target_idx]=='':                         #If target is blank, skip row                       \n",
    "        continue\n",
    "    else:\n",
    "        target.append(float(row[target_idx]))       #If pre-binned class, change float to int\n",
    "\n",
    "    #Load row into temp array, cast columns  \n",
    "    temp=[]\n",
    "                 \n",
    "    for j in range(feat_start,len(header)):\n",
    "        if row[j]=='':\n",
    "            temp.append(float())\n",
    "        else:\n",
    "            temp.append(float(row[j]))\n",
    "\n",
    "    #Load temp into Data array\n",
    "    data.append(temp)\n",
    "  \n",
    "#Test Print\n",
    "print(header)\n",
    "print(len(target),len(data))\n",
    "'''for i in range(10):\n",
    "    print(target[i])\n",
    "    print(data[i])'''\n",
    "print('\\n')\n",
    "\n",
    "data_np=np.asarray(data)\n",
    "target_np=np.asarray(target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Preprocess data\n",
    "#\n",
    "##########################################\n",
    "if resampling==1:\n",
    "    if resampling_type==1:\n",
    "        data_np, target_np = SMOTE().fit_resample(data_np, target_np)\n",
    "    elif resampling_type==2:\n",
    "        data_np, target_np = ADASYN().fit_resample(data_np, target_np)\n",
    "    elif resampling_type==3:\n",
    "        rus = RandomUnderSampler(random_state=rand_st)\n",
    "        data_np, target_np = rus.fit_resample(data_np, target_np)\n",
    "    elif resampling_type==4:\n",
    "        nm1 = NearMiss(version=1)\n",
    "        data_np, target_np = nm1.fit_resample(data_np, target_np)\n",
    "    elif resampling_type==5:\n",
    "        cc = ClusterCentroids(random_state=rand_st)\n",
    "        data_np, target_np = cc.fit_resample(data_np, target_np)\n",
    "    elif resampling_type==51:\n",
    "        cc = ClusterCentroids(random_state=rand_st)\n",
    "        data_np, target_np = cc.fit_resample(data_np, target_np)\n",
    "        data_np, target_np = SMOTE().fit_resample(data_np, target_np)\n",
    "    elif resampling_type==6:\n",
    "        sme=SMOTEENN(sampling_strategy='not minority', random_state=rand_st)\n",
    "        data_np, target_np = sme.fit_resample(data_np, target_np)\n",
    "        \n",
    "\n",
    "if norm_target==1:\n",
    "    #Target normalization for continuous values\n",
    "    target_np=scale(target_np)\n",
    "\n",
    "if norm_features==1:\n",
    "    #Feature normalization for continuous values\n",
    "    data_np=scale(data_np)\n",
    "\n",
    "if binning==1:\n",
    "    #Discretize Target variable with KBinsDiscretizer\n",
    "    enc = KBinsDiscretizer(n_bins=[bin_cnt], encode='ordinal', strategy='quantile')                         #Strategy here is important, quantile creating equal bins, but kmeans prob being more valid \"clusters\"\n",
    "    target_np_bin = enc.fit_transform(target_np.reshape(-1,1))\n",
    "\n",
    "    #Get Bin min/max\n",
    "    temp=[[] for x in range(bin_cnt+1)]\n",
    "    for i in range(len(target_np)):\n",
    "        for j in range(bin_cnt):\n",
    "            if target_np_bin[i]==j:\n",
    "                temp[j].append(target_np[i])\n",
    "\n",
    "    for j in range(bin_cnt):\n",
    "        print('Bin', j, ':', min(temp[j]), max(temp[j]), len(temp[j]))\n",
    "    print('\\n')\n",
    "\n",
    "    #Convert Target array back to correct shape\n",
    "    target_np=np.ravel(target_np_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--FEATURE SELECTION ON-- \n",
      "\n",
      "Wrapper Select - Random Forest: \n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Feature Selection\n",
    "#\n",
    "##########################################\n",
    "\n",
    "#Low Variance Filter\n",
    "if lv_filter==1:\n",
    "    print('--LOW VARIANCE FILTER ON--', '\\n')\n",
    "    \n",
    "    #LV Threshold\n",
    "    sel = VarianceThreshold(threshold=0.5)                                      #Removes any feature with less than 20% variance\n",
    "    fit_mod=sel.fit(data_np)\n",
    "    fitted=sel.transform(data_np)\n",
    "    sel_idx=fit_mod.get_support()\n",
    "\n",
    "    #Get lists of selected and non-selected features (names and indexes)\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "\n",
    "    print('Selected', temp)\n",
    "    print('Features (total, selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "\n",
    "    #Filter selected columns from original dataset\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index\n",
    "\n",
    "\n",
    "#Feature Selection\n",
    "if feat_select==1:\n",
    "    '''Three steps:\n",
    "       1) Run Feature Selection\n",
    "       2) Get lists of selected and non-selected features\n",
    "       3) Filter columns from original dataset\n",
    "       '''\n",
    "    \n",
    "    print('--FEATURE SELECTION ON--', '\\n')\n",
    "    \n",
    "    ##1) Run Feature Selection #######\n",
    "    if fs_type==1:\n",
    "        #Stepwise Recursive Backwards Feature removal\n",
    "        if binning==1:\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "            sel = RFE(clf, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "        if binning==0:\n",
    "            rgr = RandomForestRegressor(n_estimators=500, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "            sel = RFE(rgr, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        print(sel.ranking_)\n",
    "        sel_idx=fit_mod.get_support()      \n",
    "\n",
    "    if fs_type==2:\n",
    "        #Wrapper Select via model\n",
    "        if binning==1:\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "            sel = SelectFromModel(clf, prefit=False, threshold='mean', max_features=None)                                                           #to select only based on max_features, set to integer value and set threshold=-np.inf\n",
    "            print ('Wrapper Select - Random Forest: ')\n",
    "        if binning==0:\n",
    "            rgr = RandomForestRegressor(n_estimators=200, max_features=.33, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "            sel = SelectFromModel(rgr, prefit=False, threshold='mean', max_features=None)\n",
    "            print ('Wrapper Select - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)    \n",
    "        sel_idx=fit_mod.get_support()\n",
    "\n",
    "    if fs_type==3:       \n",
    "        if binning==1:                                                              ######Only work if the Target is binned###########\n",
    "            #Univariate Feature Selection - Chi-squared\n",
    "            sel=SelectKBest(mutual_info_classif, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)                                         #will throw error if any negative values in features, so turn off feature normalization, or switch to mutual_info_classif\n",
    "            print ('Univariate Feature Selection - Mutual Info: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        if binning==0:                                                              ######Only work if the Target is continuous###########\n",
    "            #Univariate Feature Selection - Mutual Info Regression\n",
    "            sel=SelectKBest(mutual_info_regression, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)\n",
    "            print ('Univariate Feature Selection - Mutual Info: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        #Print ranked variables out sorted\n",
    "        temp=[]\n",
    "        scores=fit_mod.scores_\n",
    "        for i in range(feat_start, len(header)):            \n",
    "            temp.append([header[i], float(scores[i-feat_start])])\n",
    "\n",
    "        print('Ranked Features')\n",
    "        temp_sort=sorted(temp, key=itemgetter(1), reverse=True)\n",
    "        for i in range(len(temp_sort)):\n",
    "            print(i, temp_sort[i][0], ':', temp_sort[i][1])\n",
    "        print('\\n')\n",
    "    \n",
    "    if fs_type==4:\n",
    "        #Full-blown Wrapper Select (from any kind of ML model)        \n",
    "        if binning==1:                                                              ######Only work if the Target is binned###########\n",
    "            start_ts=time.time()\n",
    "            sel_idx=[]                                                                                      #Empty array to hold optimal selected feature set\n",
    "            best_score=0                                                                                    #For classification compare Accuracy or AUC, higher is better, so start with 0\n",
    "            feat_cnt=len(data_np[0])\n",
    "            #Create Wrapper model\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)                                 #This could be any kind of classifier model\n",
    "      \n",
    "        if binning==0:                                                              ######Only work if the Target is continuous###########\n",
    "            start_ts=time.time()\n",
    "            sel_idx=[]                                                                                      #Empty array to hold optimal selected feature set\n",
    "            best_score=sys.float_info.max                                                                   #For regression compare RMSE, lower is better, so start with max sys float value\n",
    "            feat_cnt=len(data_np[0])\n",
    "            #Create Wrapper model\n",
    "            rgr = SVR(C=1.0, kernel='linear', gamma=0.1)                    #This could be any kind of regressor model         \n",
    "        \n",
    "        #Loop thru feature sets\n",
    "        roll_idx=0\n",
    "        combo_ctr=0\n",
    "        feat_arr=[0 for col in range(feat_cnt)]                                         #Initialize feature array\n",
    "        for idx in range(feat_cnt):\n",
    "            roll_idx=idx\n",
    "            feat_space_search(feat_arr, idx)                                           #Recurse\n",
    "            feat_arr=[0 for col in range(feat_cnt)]                                     #Reset feature array after each iteration\n",
    "        \n",
    "        print('# of Feature Combos Tested:', combo_ctr)\n",
    "        print(best_score, sel_idx, len(data_np[0]))\n",
    "        print(\"Wrapper Feat Sel Runtime:\", time.time()-start_ts)\n",
    "\n",
    "    ##2) Get lists of selected and non-selected features (names and indexes) #######\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "    print('Selected', temp)\n",
    "    print('Features (total/selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "            \n",
    "                \n",
    "    ##3) Filter selected columns from original dataset #########\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Train SciKit Models\n",
    "#\n",
    "##########################################\n",
    "\n",
    "print('--ML Model Output--', '\\n')\n",
    "\n",
    "\n",
    "model=[]\n",
    "modelRMSE=[]\n",
    "modelExplVar=[]\n",
    "runtimes=[]\n",
    "####Cross-Val Classifiers####\n",
    "if binning==0 and cross_val==1:\n",
    "    #Setup Crossval classifier scorers\n",
    "    scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'} \n",
    "    \n",
    "    #models    \n",
    "    \n",
    "    svr0 = SVR(kernel='rbf', C=1.0, gamma=0.1)\n",
    "    svr1 = SVR(kernel='linear', C=1.0, gamma=0.1)\n",
    "    svr2 = SVR(kernel='poly', C=1.0, gamma=0.1)\n",
    "    svr3 = SVR(kernel='sigmoid', C=1.0, gamma=0.1)\n",
    "    \n",
    "    sgd0 = SGDRegressor(alpha=0.0001, epsilon=0.1, max_iter=1000, tol=.0001, random_state=rand_st)\n",
    "    sgd1 = SGDRegressor(alpha=0.0001, epsilon=0.1, max_iter=1000, tol=.0001, penalty='l1', random_state=rand_st)\n",
    "    sgd2 = SGDRegressor(alpha=0.0001, epsilon=0.1, max_iter=1000, tol=.0001, penalty='elasticnet', random_state=rand_st)\n",
    "    sgd3 = SGDRegressor(alpha=0.0001, epsilon=0.1, max_iter=1000, tol=.0001, penalty='none', random_state=rand_st)\n",
    "    \n",
    "    rf0 = RandomForestRegressor(n_estimators=10, random_state=rand_st)\n",
    "    rf1 = RandomForestRegressor(n_estimators=100, max_depth=None, max_features=0.2, max_leaf_nodes=200, min_samples_leaf=5, min_samples_split=10, criterion='entropy', random_state=rand_st)\n",
    "    rf2 = RandomForestRegressor(n_estimators=100, max_depth=None, max_features=0.3, max_leaf_nodes=200, min_samples_leaf=5, min_samples_split=10, criterion='gini', random_state=rand_st)\n",
    "    rf3 = RandomForestRegressor(n_estimators=500, max_depth=None, max_features=0.3, max_leaf_nodes=200, min_samples_leaf=5, min_samples_split=10, criterion='entropy', random_state=rand_st)\n",
    "    rf4 = RandomForestRegressor(n_estimators=100, max_depth=None, max_features=0.2, max_leaf_nodes=200, min_samples_leaf=2, min_samples_split=5, criterion='entropy', random_state=rand_st)\n",
    "    \n",
    "    adb0 = AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=rand_st)\n",
    "    adb1 = AdaBoostRegressor(base_estimator=None, n_estimators=100, learning_rate=1.0, algorithm='SAMME.R', random_state=rand_st)\n",
    "    adb2 = AdaBoostRegressor(base_estimator=None, n_estimators=500, learning_rate=1.0, algorithm='SAMME.R', random_state=rand_st)\n",
    "    adb3 = AdaBoostRegressor(base_estimator=None, n_estimators=1000, learning_rate=1.0, algorithm='SAMME.R', random_state=rand_st)\n",
    "    adb4 = AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=0.5, algorithm='SAMME.R', random_state=rand_st)\n",
    "    \n",
    "    bc0 = BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, random_state=rand_st)\n",
    "    bc1 = BaggingRegressor(base_estimator=None, n_estimators=100, max_samples=1.0, max_features=1.0, random_state=rand_st)\n",
    "    bc2 = BaggingRegressor(base_estimator=None, n_estimators=500, max_samples=1.0, max_features=1.0, random_state=rand_st)\n",
    "    bc3 = BaggingRegressor(base_estimator=None, n_estimators=1000, max_samples=1.0, max_features=1.0, random_state=rand_st)\n",
    "    bc4 = BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=0.3, random_state=rand_st)\n",
    "\n",
    "    et0 = ExtraTreesRegressor(n_estimators=10, random_state=rand_st)\n",
    "    et1 = ExtraTreesRegressor(n_estimators=100, max_features=0.4, max_leaf_nodes=200, criterion='gini', random_state=rand_st)\n",
    "    et2 = ExtraTreesRegressor(n_estimators=100, max_features=0.4, max_leaf_nodes=200, criterion='entropy', random_state=rand_st)\n",
    "    et3 = ExtraTreesRegressor(n_estimators=500, max_features=0.4, max_leaf_nodes=200, criterion='gini', random_state=rand_st)\n",
    "    et4 = ExtraTreesRegressor(n_estimators=100, max_features=0.2, max_leaf_nodes=200, criterion='gini', random_state=rand_st)\n",
    "    \n",
    "    gb0 = GradientBoostingRegressor(n_estimators=10, random_state=rand_st)    \n",
    "    gb1 = GradientBoostingRegressor(n_estimators=10, loss='deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "    gb2 = GradientBoostingRegressor(n_estimators=10, loss='exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "    gb3 = GradientBoostingRegressor(n_estimators=100, loss='exponential', learning_rate=0.01, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "    gb4 = GradientBoostingRegressor(n_estimators=100, loss='exponential', learning_rate=0.01, max_depth=4, min_samples_split=2, random_state=rand_st)\n",
    "    \n",
    "    \n",
    "    \n",
    "    models={'svr0':lgr0,\n",
    "            'svr1':lgr1,\n",
    "            'svr2':lgr2,\n",
    "            'svr3':lgr3,\n",
    "            \n",
    "            'sgd0':sgd0,\n",
    "            'sgd1':sgd1,\n",
    "            'sgd2':sgd2,\n",
    "            'sgd3':sgd3,\n",
    "            \n",
    "            'rf0':rf0,\n",
    "            'rf1':rf1,\n",
    "            'rf2':rf2,\n",
    "            'rf3':rf3,\n",
    "            'rf4':rf4,\n",
    "            \n",
    "            'adb0':adb0,\n",
    "            'adb1':adb1,\n",
    "            'adb2':adb2,\n",
    "            'adb3':adb3,\n",
    "            'adb4':adb4,\n",
    "            \n",
    "            'bc0':bc0,\n",
    "            'bc1':bc1,\n",
    "            'bc2':bc2,\n",
    "            'bc3':bc3,\n",
    "            'bc4':bc4,\n",
    "            \n",
    "            'et0':et0,\n",
    "            'et1':et1,\n",
    "            'et2':et2,\n",
    "            'et3':et3,\n",
    "            'et4':et4,\n",
    "            \n",
    "            'gbc0':gbc0,\n",
    "            'gbc1':gbc1,\n",
    "            'gbc2':gbc2,\n",
    "            'gbc3':gbc3,\n",
    "            'gbc4':gbc4,}\n",
    "    \n",
    "    \n",
    "    #Scikit Model Iterations - Cross Val\n",
    "    \n",
    "    \n",
    "    for key, value in models.items():\n",
    "        start_ts=time.time()   \n",
    "        scores = cross_validate(value, data_np, target_np, scoring=scorers, cv=5)\n",
    "        scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['Neg_MSE']])\n",
    "        print(key+\" RMSE: %0.2f (+/- %0.2f)\" % (scores_RMSE.mean(), scores_RMSE.std() * 2))        \n",
    "        if bin_cnt<=2:\n",
    "            scores_Expl_Var = scores['expl_var']\n",
    "            print(key+\" Expl_Var: %0.2f (+/- %0.2f)\" % (scores_Expl_Var.mean(), scores_Expl_Var.std() * 2))                           \n",
    "        print(\"CV Runtime:\", time.time()-start_ts,\"\\n\")\n",
    "        model.append(key)\n",
    "        modelRMSE.append(scores_RMSE.mean())\n",
    "        modelExplVar.append(scores_Expl_Var.mean())\n",
    "        runtimes.append(time.time()-start_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['model','modelRMSE','modelExplVar','runtimes']\n",
    "fields=list(zip(model,modelRMSEs,modelExplVar,runtimes))\n",
    "output1=pd.DataFrame(fields,columns=labels)\n",
    "output1.plot.scatter(x='modelExplVar',y='modelRMSE',s=output1['runtimes']*2, alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1.to_csv(\"round_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Train Best Models\n",
    "#\n",
    "##########################################\n",
    "\n",
    "print('--ML Model Output--', '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['model','modelRMSE','modelExplVar','runtimes']\n",
    "fields=list(zip(model,modelRMSEs,modelExplVar,runtimes))\n",
    "output2=pd.DataFrame(fields,columns=labels)\n",
    "output2.plot.scatter(x='modelExplVar',y='modelRMSE',s=output1['runtimes']*2, alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.to_csv(\"round_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Train Best Models (again)\n",
    "#\n",
    "##########################################\n",
    "\n",
    "print('--ML Model Output--', '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['model','modelRMSE','modelExplVar','runtimes']\n",
    "fields=list(zip(model,modelRMSEs,modelExplVar,runtimes))\n",
    "output3=pd.DataFrame(fields,columns=labels)\n",
    "output3.plot.scatter(x='modelExplVar',y='modelRMSE',s=output1['runtimes']*2, alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3.to_csv(\"round_3.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
